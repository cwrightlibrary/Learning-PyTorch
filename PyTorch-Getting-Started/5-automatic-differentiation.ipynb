{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8aade167",
   "metadata": {},
   "source": [
    "# [PyTorch - Learning the Basics](https://pytorch.org/tutorials/beginner/basics/intro.html)\n",
    "\n",
    "In part five we'll cover automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ee4bfd",
   "metadata": {},
   "source": [
    "## Automatic differentiation with `torch.autograd`\n",
    "\n",
    "**Back propagation** is the most frequently used algorithm when training neural networks. In back propagation, parameters, or model weights, are adjusted according to the **gradient** of the loss function with respect to the given parameter. Here's a quick overview of some terms for clarification:\n",
    "\n",
    "- The **Loss function** is a formula that measures **how bad the model's prediction is** compared to the actual target. It's essentially a score that we want to be low, it's best to minimize the loss function during training.\n",
    "- The **gradient** is the **slope of the loss function** with respect to the model's parameters (weights and biases). We can use it to learn **how to change the parameters** to reduce the loss. If the gradient is positive, we want to decrease the weight and if it's negative, we want to increase it. It's calculated using **calculus** (specifically, partial derivatives).\n",
    "- **Back propagation** is the algorithm used to **efficiently compute all gradients** of the loss with respect to every weight in the network. In it, we do a **forward pass**, which is used for computing predictions and loss, and a **backward pass**, where we apply the chain rule to propagate gradients from output to input. The gradients returned from back propagation are used to **update the weights**, typically via gradient descent.\n",
    "\n",
    "In PyTorch, we use the built-in differentiation engine `torch.autograd` to compute the gradients. It supports automatic computation of gradient for any computational graph.\n",
    "\n",
    "As an example, let's consider the simplest one-layer neural network, with input `x`, parameters `w` and `b`, and some loss function. It can be defined in PyTorch like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2e36ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(5)       # input tensor\n",
    "y = torch.zeros(3)      # expected output\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311f65c5",
   "metadata": {},
   "source": [
    "### Tensors, functions, and computational graph\n",
    "\n",
    "The code above defines the following **computational graph**:\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    " subgraph s1[\"Parameters\"]\n",
    "        n7[\"w\"]\n",
    "        n8[\"b\"]\n",
    "  end\n",
    "    A[\"x\"] --> n1[\"times\"]\n",
    "    n1 --> n2[\"plus\"]\n",
    "    n2 --> n3[\"z\"]\n",
    "    n3 --> n4[\"CE\"]\n",
    "    n4 --> n5[\"loss\"]\n",
    "    n6[\"y\"] --> n4\n",
    "    n7 --> n1\n",
    "    n8 --> n2\n",
    "    n7@{ shape: rounded}\n",
    "    n8@{ shape: rounded}\n",
    "    A@{ shape: rounded}\n",
    "    n1@{ shape: rounded}\n",
    "    n2@{ shape: rounded}\n",
    "    n3@{ shape: rounded}\n",
    "    n4@{ shape: rounded}\n",
    "    n5@{ shape: rounded}\n",
    "    n6@{ shape: rounded}\n",
    "```\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
